{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSOI52QyTlH0",
        "outputId": "61aff520-a321-418e-c81a-e1ff8c5e574b"
      },
      "source": [
        "!wget http://vectors.nlpl.eu/repository/20/8.zip\n",
        "\n",
        "!unzip 8.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-05 07:32:08--  http://vectors.nlpl.eu/repository/20/8.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 672739461 (642M) [application/zip]\n",
            "Saving to: ‘8.zip’\n",
            "\n",
            "8.zip               100%[===================>] 641.57M  27.4MB/s    in 24s     \n",
            "\n",
            "2021-11-05 07:32:32 (26.5 MB/s) - ‘8.zip’ saved [672739461/672739461]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IERP-zFT03X"
      },
      "source": [
        "#make an embedding\n",
        "\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "embeddings_dict = {}\n",
        "with open(\"./model.txt\", 'r',encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], \"float32\")\n",
        "            embeddings_dict[word] = vector\n",
        "        except:\n",
        "           #there is some weird encoding in a few lines:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            print(word)\n",
        "\n",
        "\n",
        "\n",
        "def find_closest_embeddings(embedding):\n",
        "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFk1bKG-Vd0H",
        "outputId": "e2f84c6a-3726-4786-a788-3df575813367"
      },
      "source": [
        "#test all the above:\n",
        "\n",
        "print(find_closest_embeddings(embeddings_dict[\"king\"])[:5])\n",
        "\n",
        "#(should give something like king, kings etc.)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['king', 'kings', 'King', 'kingdom', 'prince']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gnv3qoIrXGC7"
      },
      "source": [
        "New tasks:\n",
        "\n",
        "A:\n",
        "Play around with the embedding vectors. In particular, try comparing relationships between word vectors. For instance, is it true that 'man' relates to 'woman' as 'king' relates to 'queen'?\n",
        "\n",
        "B:\n",
        "Create a t-SNE plot like the one shown during the lecture. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMdp57vIaHpa"
      },
      "source": [
        "#Hint to B: \n",
        "# Given a set of words contained in the embedding, this will make a t-SNE plot:\n",
        "\n",
        "\n",
        "vectors = [embeddings_dict[word] for word in words]\n",
        "\n",
        "Y = tsne.fit_transform(vectors)\n",
        "\n",
        "plt.scatter(Y[:, 0], Y[:, 1])\n",
        "\n",
        "for label, x, y in zip(words, Y[:, 0], Y[:, 1]):\n",
        "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\"offset points\")\n",
        "\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}