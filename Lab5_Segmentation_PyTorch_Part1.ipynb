{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frehors/dl-exercises/blob/main/Lab5_Segmentation_PyTorch_Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REaV1GblZovs"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Object detection and segmentation\n",
        "In this exercise you will learn about Fully convolutional networks, which are fundamental for semantic segmentation.\n",
        "\n",
        "We will do some toy-samples using MNIST, but since we don't have any small semantic-annotated dataset, the core of this exercise is to understand the concepts, rather than creating perfect outputs.\n",
        "\n",
        "After solving this, you can move on and try bigger models like FCN-8s, u-net, DeepLabv3 etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Efokwdvcb436"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "\n",
        "from torch.nn import Linear, Conv2d, MaxPool2d\n",
        "from torch.nn.functional import relu, softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc8RJkIJaxap"
      },
      "source": [
        "## 1. Import MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-_9ZwQkiGQg"
      },
      "outputs": [],
      "source": [
        "from mlxtend.data import mnist_data\n",
        "import numpy as np\n",
        "import random\n",
        "num_classes = 10\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "def splitdata(X, y, Ngroups, weights=None):\n",
        "    '''\n",
        "    X, y = input data and labels\n",
        "    Ngroups = number of groups to split data into\n",
        "    weights = a list with Ngroups weights, that tell the probability of a sample ending in either of the data sets\n",
        "    '''\n",
        "    if weights is None:\n",
        "        weights = [1/Ngroups]*Ngroups\n",
        "\n",
        "    groups = np.array(random.choices(list(range(Ngroups)), weights=weights, cum_weights=None, k=len(X)))\n",
        "    return ((X[np.where(groups==g)], y[np.where(groups==g)]) for g in np.array(range(Ngroups)))\n",
        "\n",
        "\n",
        "\n",
        "X, y = mnist_data()\n",
        "X = (X/255).astype(np.float32) # Convert to interval 0:1\n",
        "y = y.astype(np.float32)\n",
        "num_classes = 10\n",
        "nchannels, rows, cols = 1, 28, 28\n",
        "\n",
        "X = np.expand_dims(X.reshape(len(X),rows,cols),1) # Add a channel-dimension\n",
        "\n",
        "#Split data in train and test\n",
        "(x_train_org, y_train_org), (x_test_org, y_test_org) = splitdata(X, y, Ngroups=2, weights=[0.8, 0.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK1n8L9Ub44A"
      },
      "outputs": [],
      "source": [
        "#Set-up data loaders\n",
        "batch_size = 32\n",
        "\n",
        "#Convert to tensors\n",
        "x_train = torch.tensor(x_train_org)\n",
        "y_train = torch.tensor(y_train_org)\n",
        "x_test = torch.tensor(x_test_org)\n",
        "y_test = torch.tensor(y_test_org)\n",
        "\n",
        "trainset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "testset = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwMozk31fnTL"
      },
      "source": [
        "Print some stats:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTvEhwcQfqcH"
      },
      "outputs": [],
      "source": [
        "# Input shape: 28 x 28 x 1 = image with one color channel\n",
        "input_shape = x_train[0].shape\n",
        "print('input_shape :',input_shape)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# to_categorical converts class indices to one-hot vectors\n",
        "print('y_train shape:', y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkOorGdZf4L6"
      },
      "source": [
        "## 2. Train a simple CNN classifier\n",
        "Set up CNN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvHAmMda0uNE"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "channels = x_train.shape[1]\n",
        "height = x_train.shape[2]\n",
        "width = x_train.shape[3]\n",
        "\n",
        "\n",
        "# define network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding='same')\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding='same')\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding='same')\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.FC1 = nn.Linear(7*7*32, 64)  \n",
        "        self.FC2 = nn.Linear(64, num_classes)  \n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.flatten(x)\n",
        "        x = self.FC1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.FC2(x)\n",
        "        #print(x.shape)\n",
        "        predictions = self.softmax(x)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44dU5LYWrMXY"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fRF_F3tb44G"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
        "#optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "nEpocs = 30 # How many times should we run over the data set?\n",
        "totaliter = 0\n",
        "net.train()\n",
        "for epoch in range(nEpocs):  # loop over the dataset multiple times\n",
        "    print('epoch',epoch)\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    #scheduler.step()\n",
        "    \n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        totaliter+=1\n",
        "        \n",
        "        \n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "\n",
        "    print('Epoch %d, Train loss: %.3f' %(epoch + 1, running_loss / 100))\n",
        "            \n",
        "    running_loss = 0.0\n",
        "    \n",
        "print('Finished Training')\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs9A2gSKhmOM"
      },
      "source": [
        "## 3. Task 1: Simple sliding window\n",
        "Sliding window  (i.e., repeating the same operation on an image at many different locations) is a core operation in many computer vision tasks.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/576/1*Mj8WKVKf_RpiAsX3SC1ZdQ.png)\n",
        "\n",
        "To perform sliding window efficiently - and to handle input images of varying shape - most modern CNNs are *fully convolutional*. The term is not uniquely defined, but at its core it addresses the issue that fully connected layers (or dense layers) require that the input vector has a fixed, predetermined shape. There are basically two solutions to this problem:\n",
        "\n",
        "1. Perform some sort of pooling operation on the last feature map of the convolutional encoder, such that the input to the fully connected layers has a fixed shape, regardless of the shape of the input image. For instance, *global average pooling* takes an CxNxN volume and reduces it to a C-dimensional vector by averaging the NxN valus of each channel.\n",
        "2. Convert the fully connected layers into convolutional layers. This allows the network to handle images of arbitrary shape. If the shape of the last feature map is CxNxN, the first step of replacing the fully connected layers with convolutional layers is to perform convolution with K different NxN sized filters to produce an output volume of shape Kx1x1. Performing successive 1x1 convolutions on this volume \"mimics\" the traditional fully connected layers. \n",
        "\n",
        "The main difference between the two approaches above is that in *1.* the output shape is always the same, whereas in *2.* the output shape increases as the shape of the input image increases. This is because *2.* performs a kind of *sliding window* operation. We will implement this later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkQUJdrN3UT4"
      },
      "source": [
        "### Create a larger test image to perform sliding window on\n",
        "To motivate why we need fully convolutional networks to perform sliding window effeciently, lets first consider simple sliding window, where each window is run through a CNN.\n",
        "\n",
        "The CNN that we trained above takes input images of size 28 x 28 and produces a 10-dimensional output vector of class probabilities. Now, suppose that the input image is three times as large (i.e., it has size 84 x 84), but the digits have the same absolute scale/shape as before. Let's generate and display such a test image using zero padding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTAL7qdze1vA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Change to select another test image\n",
        "i = 400\n",
        "img = x_test[i,:] # Dimensions are [batch_size, channels, height, width]\n",
        "\n",
        "# Pad with zeros to obtain image of size (28 + 2*padsize) = 84 if padsize = 28\n",
        "padsize = 28\n",
        "img_large = np.pad(img, ((0,0),(padsize,padsize),(padsize,padsize)))       \n",
        "       \n",
        "#Print shapes    \n",
        "print('Original shape',img.shape)\n",
        "print('Larger image shape',img_large.shape)\n",
        "\n",
        "# Display\n",
        "plt.subplot(1,2,1); plt.imshow(img.squeeze(),cmap='gray'); plt.title('Original (28 x 28)');\n",
        "plt.subplot(1,2,2); plt.imshow(img_large.squeeze(),cmap='gray'); plt.title('Larger image (84 x 84)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGon_HY0e0lT"
      },
      "source": [
        "### Run sliding window\n",
        "We should be able to detect the digit in the larger image (`img_large`) by running our current CNN model over the image in a sliding window fashion. \n",
        "The input image is 84 x 84 pixels, and the network accepts images of shape 28x28. With a stride of 1, how many times can we run the network over the input image along each spatial dimension? Call this number `N`, insert it in the code block below, and run it. \n",
        "\n",
        "Please ask me (Mads) if the task is unclear, or if you get stuck.\n",
        "\n",
        "**Note** that it will take a while to run the code block below - that's the whole point :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_RK8Uh7gp-1",
        "scrolled": false,
        "tags": []
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Initialize result array\n",
        "N = ???\n",
        "\n",
        "result = np.zeros((N,N,num_classes))\n",
        "net.eval()\n",
        "# Perform sliding window\n",
        "start = time.time()\n",
        "for x in range(N):\n",
        "  for y in range(N):\n",
        "    cropout = img_large[:,x:x+28,y:y+28]\n",
        "    output = net(torch.tensor(cropout).unsqueeze(0))\n",
        "    result[x,y,:] = output.detach().numpy()\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print('Elapsed time',elapsed)\n",
        "\n",
        "# Show result\n",
        "plt.figure(figsize=(20,6))\n",
        "for c in range(num_classes):\n",
        "  plt.subplot(1,num_classes,c+1)\n",
        "  plt.imshow(result[:,:,c],vmin=0.,vmax=1.,cmap='gray')\n",
        "  plt.title(str(c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTaxXOYNb44N"
      },
      "source": [
        "### Questions\n",
        "\n",
        "1. What are we looking at here? That is, explain what the output images (result) show.\n",
        "1. How long time did it take to run sliding window this way?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFmjkuGAb44N"
      },
      "source": [
        "## Voting the outputs\n",
        "The figure below shows the argmax (i.e., the index of the most likely class) for each spatial location. Note that the most likely class differs for different window positions. We can combine information from all spatial positions by counting the number of votes each class got.\n",
        "Also Note that since we use softmax we forces the output to sum to 1, even though the classifier is unsure (e.g., in dark areas). Moreover, big digits are more likely to be covered and will therefore be detected at more positions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9upd9PzTb44O"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1,2);\n",
        "axs\n",
        "images = []\n",
        "images.append(axs[0].imshow(img_large.squeeze(), cmap='gray'))\n",
        "images.append(axs[1].imshow(np.argmax(result,axis=2),vmin=0,vmax=9))\n",
        "fig.colorbar(images[1], ax=axs, orientation='horizontal', fraction=.1)\n",
        "\n",
        "# Class with most \"votes\"\n",
        "class_map = np.argmax(result,axis=2)\n",
        "for digit in range(10):\n",
        "  ix = np.where(class_map==digit)[0]\n",
        "  votes = len(ix)\n",
        "  print(str(digit) + ' got ' + str(votes) + ' votes')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obC47RsqjpJb"
      },
      "source": [
        "## 4. Task 2: Efficient sliding window with FCN\n",
        "Sliding window as implemented can take several seconds, even for this tiny image. To make the sliding window operation more efficient, we can convert our model to a fully convolutional network (FCN) by converting the fully connected layers to convolutional layers.\n",
        "\n",
        "In summary, the procedure involves the following steps:\n",
        "\n",
        "1. First set up an FCN equivalent to the CNN that we used above.\n",
        "2. Train the FCN on the 28 x 28 training images (`x_train`).\n",
        "3. Modify the FCN architecture such that it takes input images of arbitrary input shape (and test it on the 84 x 84 image).\n",
        "\n",
        "The latter is a two-step procedure, where we first modify the FCN architecture a little bit (by removing a flatten layer), then copy the weights from the trained network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX7DLQeq3eWs",
        "tags": []
      },
      "source": [
        "### Step 1 - set up FCN\n",
        "As a first step, convert our fully connected model into an FCN by replacing the fully connected layers with convolution layers. You can do this by filling in the missing code below (see comments to get help):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpS2qECJb44P"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "channels = x_train.shape[1]\n",
        "height = x_train.shape[2]\n",
        "width = x_train.shape[3]\n",
        "\n",
        "\n",
        "# define network\n",
        "    \n",
        "class FCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCN, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding='same')\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding='same')\n",
        "        \n",
        "        # Add three conv. layers that fulfill the requirements in the forward pass. \n",
        "\n",
        "        \n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        \n",
        "        # 1. At this point x has shape (N, 32, 7, 7)  \n",
        "        # 1.a add relu\n",
        "        \n",
        "        # 2. At this point x has shape (N, 64, 1, 1)\n",
        "        # 2.a add relu\n",
        "        \n",
        "        # 3. At this point predictions has shape (N, num_classes, 1, 1)\n",
        "        \n",
        "        \n",
        "\n",
        "        predictions = self.softmax(x) # softmax is changed to work along channel dimension\n",
        "        return predictions\n",
        "\n",
        "\n",
        "fcn = FCN()\n",
        "print(fcn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQyWluqPmIMY"
      },
      "source": [
        "### Step 2 - train FCN\n",
        "Unfortunately, it is not so straight forward to transfer the learned weights from the existing CNN (`model`) to the FCN (`fcn`). So we need to quickly train it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TviA4E30b44Q"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(fcn.parameters(), lr=0.1)\n",
        "#optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5) # decrease LR every two steps\n",
        "\n",
        "nEpocs = 30 # How many times should we run over the data set?\n",
        "totaliter = 0\n",
        "fcn.train()\n",
        "for epoch in range(nEpocs):  # loop over the dataset multiple times\n",
        "    #print('epoch',epoch)\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    #scheduler.step()\n",
        "    \n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        totaliter+=1\n",
        "        \n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize      \n",
        "        outputs = fcn(inputs)\n",
        "        outputs_reshape = outputs.view(outputs.shape[0],-1)\n",
        "        \n",
        "        loss = criterion(outputs_reshape, labels.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "\n",
        "    print('Epoch %d, Train loss: %.3f' %(epoch + 1, running_loss / 100))\n",
        "            \n",
        "    running_loss = 0.0\n",
        "print('Finished Training')\n",
        "fcn.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byntwzvimju7"
      },
      "source": [
        "#### Question\n",
        "1. Why is the `outputs.view` operation required during training, but not during test?\n",
        "1. Is there any sliding window going on here (when running `fcn`)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq6B7cQF4BYK"
      },
      "source": [
        "### Test speed (FCN vs simple slinding window)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7u1FXbwDvBW"
      },
      "source": [
        "Finally, let's verify that `fcn` performs sliding window much faster than the simple sliding window above by processing the 84 x 84 image, `img_large`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoJeYn0ODV8y"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "result_large = fcn(torch.tensor(img_large).unsqueeze(0))\n",
        "elapsed = time.time() - start\n",
        "print(result_large.shape)\n",
        "print(elapsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XHDZ-Jpmb9-"
      },
      "source": [
        "Display the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CqDXqgDrI-L"
      },
      "outputs": [],
      "source": [
        "# Show input image\n",
        "plt.figure()\n",
        "plt.imshow(img_large.squeeze(),cmap='gray')\n",
        "\n",
        "#print(np.max(result_large.detach().numpy()[:,c,:,:]))\n",
        "\n",
        "# Show result\n",
        "plt.figure(figsize=(20,6))\n",
        "for c in range(num_classes):\n",
        "  plt.subplot(1,num_classes,c+1)\n",
        "  plt.imshow(result_large.detach().numpy()[:,c,:,:].squeeze(),vmin=0.,vmax=1,cmap='gray')\n",
        "  plt.title(str(c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5J15TLJGtBj"
      },
      "source": [
        "### Questions:\n",
        "1. With simple sliding window, the output shape of `results` was NxNx10 = 56x56x10. However, when using FCN for sliding window, the output shape is 15x15x10. Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Style transfer\n",
        "\n",
        "Chapter 14.12 in [Dive into Deep Learning](https://d2l.ai/d2l-en.pdf) Shows how Neural Style Transfer can be implemented in PyTorch. Try implementing it on your own images. Try changing the weight of the style loss and the content loss. How does that affect the synthetic image?\n",
        "The code samples uses a library called d2l, which is available from here: https://github.com/d2l-ai/d2l-en/blob/master/d2l/torch.py"
      ],
      "metadata": {
        "id": "cYFXMplEcrPd"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}